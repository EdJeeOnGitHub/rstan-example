---
title: "Bayesian Modelling in RStan"
author: "Ed Jee"
date: "14/05/2019"
output: 
  ioslides_presentation:
    widescreen: true
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE,
                      warning = TRUE)
library(knitr)
```


## Contents HERE

## What is Stan?

[Stan](https://mc-stan.org/) takes the difficult parts out of Markov Chain Monte Carlo (MCMC) and statistical computation.

Essentially it's an interface to a bunch of linear algebra written in C++ and some probability density functions.

Stan lets us focus on writing model code that approximately resembles how we write Bayesian models with pen and paper.


```{r, fig.width=2, fig.height=2, fig.align='right'}
include_graphics("../img/stan_logo.png")
```

## What is RStan?

RStan is just a wrapper around Stan that lets us access Stan from R.

We don't need to use RStan, instead we could use:

- Python, PyStan.
- Shell, CMDStan.
- MATLAB, MatlabStan.
- Julia, Stan.jl.
- Stata, StataStan (if you __really__ want to).

However, RStan is the most mature option with the largest community and a rich ecosystem of supporting packages ([rstanarm](https://mc-stan.org/rstanarm/), [brms](https://mc-stan.org/users/interfaces/brms), [tidybayes](http://mjskay.github.io/tidybayes/)...).

## Why Stan?

- Stan is more flexible than alternative Bayesian offerings such as JAGS and BUGS.
- Stan's MCMC methods use __Hamiltonian Monte Carlo__ rather than Gibbs sampling or Metropolis-Hastings.
    - This is sort of a big deal.
- Models we couldn't dream of fitting with Gibbs or MH are now (relatively) easy to fit.

## Our First Model

The "hello world!" of Bayesian modelling in Stan is the [eight schools experiment](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started) (Rubin 1981, Gelman et al 2003).

We want to find the effect of specialist coaching on SAT test scores. The programme was implemented in parallel across 8 different schools:


```{r, results = "asis"}
library(dplyr)
school_data <- list(J = 8, 
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
school_data %>% 
  as_tibble() %>% 
  mutate(school = row_number()) %>% 
  select(-J) %>% 
  t() %>% 
  kable()
```


## The Model:

$$
\begin{aligned}
y_j & \sim \text{Normal}(\theta_j, \sigma_j), \  j = 1,...,8 \\
\theta_j &\sim \text{Normal}(\mu, \tau), \ j = 1,...,8 \\
 \sigma_j  & \  \text{known.}
\end{aligned}
$$
With pretty standard priors:

$$
\begin{aligned}
\mu &\sim \text{Normal}(0, 5) \\
\tau &\sim \text{Cauchy}^+(0, 5) \\
\newline
\theta_j &\sim \text{Normal}(\mu, \tau)
\end{aligned}
$$

## data

We create a `.stan` file named `eight_school_centred.stan` (you can find it under the `stan/` folder in the repo).

Each Stan file contains a `data` modelling block:
```{stan, echo = TRUE,  eval = FALSE, output.var = "data_block"}
data {
  int<lower=0> J; // N schools
  real y[J]; // treatment indexed by J
  real<lower=0> sigma[J]; // s.e. indexed by J
}
```

We need to set up our data in R accordingly:
```{r, echo = TRUE}
school_data <- list(J = 8, 
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
```



## parameters

The parameters model block is pretty simple too:

```{stan, echo = TRUE, eval = FALSE, output.var = "param_block"}

parameters {
  real mu;
  real<lower=0> tau;
  real theta[J];
}


```


## model

Here we specify the likelihood and our priors:

```{stan, echo = TRUE, eval = FALSE, output.var="model_block"}

model {
  // Our "population" priors
  mu ~ normal(0, 5); // location
  tau ~ cauchy(0, 5); // scale
  
  // The "lower rung" of the hierarchy ladder
  theta ~ normal(mu, tau); 
  
  // Our likelihood:
  y ~ normal(theta, sigma); 
}


```

## Fitting the Model




```{r, echo = TRUE, eval = TRUE}
library(rstan)
options(mc.cores = parallel::detectCores()) # How many cores to use. 
rstan_options(auto_write = TRUE) # Saves time on model re-compilation.
set.seed(1234)

# Compile the model
eight_school_centred_stan <- stan_model(file = "stan/eight_school_centred.stan")
# Fit the model
model_draws_centred <- sampling(eight_school_centred_stan,
                        data = school_data,
                        iter = 10000,
                        chains = 4)
```

## Inspecting the Model


```{r, echo = TRUE, eval = FALSE}
library(shinystan)
launch_shinystan(model_draws_centred)
```


## Fixing the Model

Divergent transition warnings is common when fitting hierarchical models.

We have three options:

  - Reparametrise the model.
  - Increase adapt_delta.
  - Try different priors.



## Non-Centred Parametrisation

A lot of things we do in Bayesian modelling is with an eye towards numerical optimisation - this is one of them:


```{stan eval = FALSE, echo = TRUE, output.var = "ncp_param"}
transformed parameters {
  real theta[J];
  for (j in 1:J){
    theta[j] = mu + tau * theta_tilde[j]; // theta's final form
  }
}
model {
  mu ~ normal(0, 5);
  tau ~ cauchy(0, 5);
  
  theta_tilde ~ normal(0, 1); // Will be N(mu, tau) after transform
  
  y ~ normal(theta, sigma);
}
```

## 

Fitting the non-centred model and bumping `adapt_delta` up:
```{r, echo = TRUE}
eight_school_non_centred_stan <- stan_model(file = "stan/eight_school_non_centred.stan")
model_draws_ncp <- sampling(eight_school_non_centred_stan,
                        data = school_data,
                        iter = 10000,
                        chains = 4,
                        control = list(adapt_delta = 0.95))
```

## Reparametrisation Solves the Divergent Transition Problem

```{r, fig.align = "center", warning = FALSE, message=FALSE}
library(tidybayes)
library(purrr)
dt_c <- model_draws_centred %>% 
  spread_draws(theta[j], tau) %>% 
  filter(j == 1)

sampler_params <- get_sampler_params(model_draws_centred,  inc_warmup=FALSE) %>%
  map_df(as_tibble)

dt_c <- bind_cols(dt_c,
                  sampler_params) %>% 
  mutate(divergent = factor(divergent__),
         model = "Centred")


dt_ncp <- model_draws_ncp %>% 
  spread_draws(theta[j], tau) %>% 
  filter(j == 1)
sampler_params_ncp <-  get_sampler_params(model_draws_ncp,  inc_warmup=FALSE) %>%
  map_df(as_tibble)


dt_ncp <- bind_cols(dt_ncp,
                  sampler_params_ncp) %>% 
  mutate(divergent = factor(divergent__),
         model = "NCP")

dt_plot <- bind_rows(dt_c,
                     dt_ncp)
dt_plot %>% 
  ggplot(aes(x = theta, y = log(tau), colour = divergent, alpha = divergent)) +
  geom_point() +
  scale_color_manual(values = c("grey","red")) +
  theme_minimal() +
  facet_wrap(~model) 
```

## Model Output

RStan's default output:
```{r, echo = TRUE}
plot(model_draws_ncp, pars = "theta")
```

## Tidybayes


```{r}
tidy_no_pooling <- school_data %>% 
  as_tibble() %>% 
  mutate(j = factor(row_number()),
         conf.high = y + sigma*1.96,
         conf.low = y - sigma*1.96,
         model = "No Pooling") %>% 
  select(-J,
         theta = y)
```


```{r, echo = TRUE, eval = FALSE}
library(tidybayes)
library(ggstance)

tidy_ncp_draws <- model_draws_ncp %>% 
  spread_draws(theta[j])  # Tidy model draws.

tidy_ncp_draws %>% 
  median_qi() %>% 
  to_broom_names() %>% 
  mutate(model = "BHM",
         j = factor(j)) %>% 
  bind_rows(tidy_no_pooling) %>% # Adding original data
  ggplot(aes(x = theta, xmin = conf.low, xmax = conf.high,
             y = j,
             colour = model)) +
  geom_pointrangeh(position = position_dodgev(height = 0.3)) +
  theme_minimal() +
  geom_vline(xintercept = 0, linetype = "longdash") 
```

## No Pooling vs Partial Pooling

```{r, echo = FALSE, eval = TRUE}
library(tidybayes)
library(ggstance)

tidy_ncp_draws <- model_draws_ncp %>% 
  spread_draws(theta[j])  # Tidy model draws.

tidy_ncp_draws %>% 
  median_qi() %>% 
  to_broom_names() %>% 
  mutate(model = "BHM",
         j = factor(j)) %>% 
  bind_rows(tidy_no_pooling) %>% # Adding original data
  ggplot(aes(x = theta, xmin = conf.low, xmax = conf.high,
             y = j,
             colour = model)) +
  geom_pointrangeh(position = position_dodgev(height = 0.3)) +
  theme_minimal() +
  geom_vline(xintercept = 0, linetype = "longdash") 
```


## ggridges
```{r}
library(ggridges)
tidy_ncp_draws %>% 
  ungroup() %>% 
  mutate(j = factor(j)) %>% 
  ggplot(aes(x = theta,
             y = j,
             fill = j)) +
  stat_density_ridges(scale = 2,
                      quantile_lines = TRUE,
                      quantiles = c(0.025,
                                    0.5,
                                    0.975),
                      alpha = 0.7) +
  theme_ridges() +
  labs(caption = "Note: 95% credibility intervals and posterior median displayed.")
```

[ggmcmc](http://xavier-fim.net/post/resources-for-ggmcmc/) is a good alternative too (+ diagnostic plots). 

## Posterior Predictive Checks

Bayesian analysis fully characterises the posterior distribution.

  - This means we can simulate draws from the posterior and inspect goodness-of-fit.
  
```{stan eval = FALSE, echo = TRUE, output.var = "gen_quan"}
generated quantities {
 real Y_rep[J];
 for (j in 1:J){
  Y_rep[j] = normal_rng(theta, sigma);
 }
}
```






## Bayesian Workflow


## Fitting More Models

Regression example Blattman

## OLS LKJ
## OLS QR decomposition
## Bayesian Regularisation MAYBE


Regression examples rstanarm brms

Regression example rugs

##

##



## R Markdown

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Slide with Bullets

- Bullet 1
- Bullet 2
- Bullet 3

## Slide with R Output

```{r cars, echo = TRUE}
summary(cars)
```

## Slide with Plot

```{r pressure}
plot(pressure)
```


## Further Reading/References

![tidybayes](http://mjskay.github.io/tidybayes/)